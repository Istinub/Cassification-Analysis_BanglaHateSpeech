{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VoGQVVad6b6F"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#For data preprocessing\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "#For NLP\n",
    "import re\n",
    "import nltk\n",
    "import nltk.corpus\n",
    "\n",
    "from bnlp import NLTKTokenizer\n",
    "bnltk = NLTKTokenizer()\n",
    "from bnlp.corpus import stopwords\n",
    "from bnlp.corpus.util import remove_stopwords\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "\n",
    "#train split and fit models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "#model selection\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "import os\n",
    "for firname, _, filenames in os.walk('/kaggle/input'):\n",
    "  for filename in filenames:\n",
    "    print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wq0E1zqO6jtk"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"bangla_hate.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "VGY9I3QqWIvR",
    "outputId": "08719f79-0a3f-4ed9-fddc-40986a1543f2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/rkarim/.local/lib/python3.7/site-packages/pandas/compat/__init__.py:97: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from numpy import newaxis as na\n",
    "import keras\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F15gy7k26PK_"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Activation, Conv2D, Input, Embedding, Reshape, MaxPool2D, Concatenate, Flatten, Dropout, Dense, Conv1D\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from keras.layers import Embedding\n",
    "# preparing input to our model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# keras layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Bidirectional, LSTM, GRU, Dense\n",
    "# preparing input to our model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# keras layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i1teyQSW6PLA",
    "outputId": "a0c6ddee-ed56-4dae-bafb-fdc572fe3a0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5887 entries, 0 to 5886\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    5887 non-null   object\n",
      " 1   label   5887 non-null   object\n",
      " 2   target  5887 non-null   int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 138.1+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>৭৫ হামলার আসামী কে এই গডফাদার কালা মন্দির?</td>\n",
       "      <td>Personal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>রাজাকারদের মন্ত্রী বানানোর দায়ে তার আরও কঠিন ব...</td>\n",
       "      <td>Political</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>বৃটিশ নাগরিকদের বাংলাদেশ ভ্রমনে সতর্কতা জারী ২...</td>\n",
       "      <td>Geopolitical</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>বলদ নয়াদিগন্ত কালকে রিপোর্ট করলো হিলারী শপথ ন...</td>\n",
       "      <td>Political</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>অসভ্য বর্বর বেহায়া ভোট ডাকাত সকল প্রকার খারাপ ...</td>\n",
       "      <td>Geopolitical</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text         label  target\n",
       "0         ৭৫ হামলার আসামী কে এই গডফাদার কালা মন্দির?      Personal       0\n",
       "1  রাজাকারদের মন্ত্রী বানানোর দায়ে তার আরও কঠিন ব...     Political       1\n",
       "2  বৃটিশ নাগরিকদের বাংলাদেশ ভ্রমনে সতর্কতা জারী ২...  Geopolitical       3\n",
       "3   বলদ নয়াদিগন্ত কালকে রিপোর্ট করলো হিলারী শপথ ন...     Political       1\n",
       "4  অসভ্য বর্বর বেহায়া ভোট ডাকাত সকল প্রকার খারাপ ...  Geopolitical       3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's import and prep the datasets\n",
    "\n",
    "train=pd.read_csv('bangla_hate.csv', sep=',', encoding='utf-8')\n",
    "test=pd.read_csv('test.csv', sep=',', encoding='utf-8')\n",
    "\n",
    "train.info()\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x3PqM3O76PLA",
    "outputId": "dc32d28b-53db-47cb-fa87-c592fb8ba4c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1200 entries, 0 to 1199\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    1200 non-null   object\n",
      " 1   label   1200 non-null   object\n",
      " 2   target  1200 non-null   int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 28.2+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>মুশরিকদের হত্যা কর যেখানেই তাদের পাও এবং তাদের...</td>\n",
       "      <td>Religious</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>কোথায় মহারানী আর কোথায় চুতমারানী?</td>\n",
       "      <td>Personal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>হিন্দু বা কাফের দের সাতে হাত মিলিয়ে সারা দুনিয়...</td>\n",
       "      <td>Religious</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>নিজের তো সব গেছেই এখন মেয়েটার জীবন শেষ করতে আক...</td>\n",
       "      <td>Personal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>এই ছবি পুরো জাতীকে লজ্জা দেয়, আজ ঢাকায় ট্রাক থ...</td>\n",
       "      <td>Political</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text      label  target\n",
       "0  মুশরিকদের হত্যা কর যেখানেই তাদের পাও এবং তাদের...  Religious       2\n",
       "1                 কোথায় মহারানী আর কোথায় চুতমারানী?    Personal       0\n",
       "2  হিন্দু বা কাফের দের সাতে হাত মিলিয়ে সারা দুনিয়...  Religious       2\n",
       "3  নিজের তো সব গেছেই এখন মেয়েটার জীবন শেষ করতে আক...   Personal       0\n",
       "4  এই ছবি পুরো জাতীকে লজ্জা দেয়, আজ ঢাকায় ট্রাক থ...  Political       1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.info()\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3HMIyPJO6PLB"
   },
   "outputs": [],
   "source": [
    "import csv \n",
    "\n",
    "stop_words='stopwords-bn.txt'\n",
    "text_data=[]\n",
    "\n",
    "with open(stop_words,'r',encoding='utf-8') as temp_output_file:\n",
    "    reader=csv.reader(temp_output_file, delimiter='\\n')\n",
    "    for row in reader:\n",
    "        text_data.append(row)\n",
    "stop_word_list=[x[0] for x in text_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g5VScc0H6PLB"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords  \n",
    "from nltk.tokenize import word_tokenize  \n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "  \n",
    "stop_words = set(stop_word_list)  \n",
    "\n",
    "def textCleaner(example_sent): \n",
    "    word_tokens = word_tokenize(example_sent)  \n",
    "    filtered_train = TreebankWordDetokenizer().detokenize(word_tokens)\n",
    "\n",
    "    return filtered_train\n",
    "\n",
    "filtered_test = test['text'].apply(textCleaner)\n",
    "filtered_train = train['text'].apply(textCleaner)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dZpBnOqq6PLD",
    "outputId": "5666f905-f196-4381-bc01-d56015b64030"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       মুশরিকদের হত্যা কর যেখানেই তাদের পাও এবং তাদের...\n",
       "1                       কোথায় মহারানী আর কোথায় চুতমারানী?\n",
       "2       হিন্দু বা কাফের দের সাতে হাত মিলিয়ে সারা দুনিয়...\n",
       "3       নিজের তো সব গেছেই এখন মেয়েটার জীবন শেষ করতে আক...\n",
       "4       এই ছবি পুরো জাতীকে লজ্জা দেয়, আজ ঢাকায় ট্রাক থ...\n",
       "                              ...                        \n",
       "1195    কুদ্দুস বলতো গরু আমাদের কি দেয়? স্যার, গরু আমা...\n",
       "1196    ওরা দুজন আগের জন্মে মুসলমান ছিল এই জন্মে হিন্দ...\n",
       "1197    মুহম্মদ দুনিয়াতেই মাগিবাজি লম্পটগিরি করল তার উ...\n",
       "1198           কেনেন, লাগান, ভুলে যান। আরাম করে চুদে দেন।\n",
       "1199    এক পক্ষ হয়রানির শিকার সময়ের সাহসী শিক্ষক হালিম...\n",
       "Name: text, Length: 1200, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u2E5ZWMk6PLD",
    "outputId": "b39e82b8-4290-4020-8e17-50a81761c969"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Personal        2561\n",
       "Geopolitical    1691\n",
       "Religious        908\n",
       "Political        727\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H0Ctr7MR6PLE",
    "outputId": "310759c8-e150-46a3-a99e-e924e423112b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2561\n",
       "3    1691\n",
       "2     908\n",
       "1     727\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mv-65QA77W37"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(filtered_train, train['target'].values, test_size=0.2, random_state=42)\n",
    "x_test, y_test = filtered_test, test['target'].values\n",
    "max_features = 10000\n",
    "\n",
    "maxlen = 20\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RSGwCath6PLF",
    "outputId": "e242e988-45c8-4f62-9153-acaf558fecb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words : 15677\n",
      "Shape of training data tensor: (4709, 20)\n",
      "Shape of training label tensor: (1178,)\n",
      "Shape of val data tensor: (1178, 20)\n",
      "Shape of val label tensor: (1178,)\n",
      "Shape of test data tensor: (1200, 20)\n",
      "Shape of test label tensor: (1200,)\n"
     ]
    }
   ],
   "source": [
    "tokenizer  = Tokenizer(num_words = max_features)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "train_seq =  tokenizer.texts_to_sequences(x_train)\n",
    "val_seq =  tokenizer.texts_to_sequences(x_val)\n",
    "test_seq = tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print(\"Unique words : {}\".format(len(word_index)))\n",
    "\n",
    "x_train = pad_sequences(train_seq, maxlen=maxlen, dtype='int32', padding='post') \n",
    "x_val = pad_sequences(val_seq, maxlen=maxlen, dtype='int32', padding='post') \n",
    "x_test = pad_sequences(test_seq, maxlen=maxlen, dtype='int32', padding='post') \n",
    "\n",
    "y_train = np.asarray(y_train)\n",
    "y_val = np.asarray(y_val)\n",
    "y_test = np.asarray(y_test)\n",
    "\n",
    "\n",
    "print('Shape of training data tensor:', x_train.shape)\n",
    "print('Shape of training label tensor:', y_val.shape)\n",
    "\n",
    "print('Shape of val data tensor:', x_val.shape)\n",
    "print('Shape of val label tensor:', y_val.shape)\n",
    "\n",
    "print('Shape of test data tensor:', x_test.shape)\n",
    "print('Shape of test label tensor:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MHlMOEyF6PLF"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import permutations\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LDtMsB0b6PLF"
   },
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "word_vectors = Word2Vec.load('Bengali_FastText_DIM20.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y-VA0VNW6PLF"
   },
   "outputs": [],
   "source": [
    "NUM_WORDS=20000\n",
    "EMBEDDING_DIM=20\n",
    "\n",
    "vocabulary_size=len(tokenizer.word_index)+1\n",
    "word_index=tokenizer.word_index\n",
    "embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i>=NUM_WORDS:\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector=word_vectors[word]\n",
    "        embedding_matrix[i]=embedding_vector\n",
    "    except KeyError:\n",
    "        embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n",
    "\n",
    "del(word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W6wUVvq36PLG",
    "outputId": "4adad861-58f2-4313-a39c-c43d735fbc2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "embedding_layer=Embedding(vocabulary_size, EMBEDDING_DIM, weights=[embedding_matrix],trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EVsIwJ2N6PLG"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Embedding\n",
    "from keras.initializers import Constant\n",
    "from keras.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AYObfS_d6PLG"
   },
   "outputs": [],
   "source": [
    "def BiLSTM_Model():\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    model.add(SpatialDropout1D(0.5))\n",
    "    model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Bidirectional(LSTM(32)))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(Dense(units=128, activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(units=4, activation='softmax'))\n",
    "    model.compile(loss = 'sparse_categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    return model     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L6YY6dPl6PLG",
    "outputId": "9c863905-f7d3-4ce2-9817-2a0c3b10c9f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 20)          313560    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, None, 20)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, None, 128)         43520     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 64)                41216     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 407,132\n",
      "Trainable params: 93,572\n",
      "Non-trainable params: 313,560\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = BiLSTM_Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zbZuq10i6PLH",
    "outputId": "9284d342-a65d-4108-cc93-dcca6b47d636"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 4709 samples, validate on 1178 samples\n",
      "Epoch 1/50\n",
      "4709/4709 [==============================] - 5s 1ms/step - loss: 1.1624 - acc: 0.5271 - val_loss: 1.0279 - val_acc: 0.5857\n",
      "Epoch 2/50\n",
      "4709/4709 [==============================] - 3s 659us/step - loss: 1.0607 - acc: 0.5776 - val_loss: 0.9618 - val_acc: 0.6171\n",
      "Epoch 3/50\n",
      "4709/4709 [==============================] - 3s 662us/step - loss: 1.0454 - acc: 0.5772 - val_loss: 0.9382 - val_acc: 0.6375\n",
      "Epoch 4/50\n",
      "4709/4709 [==============================] - 3s 662us/step - loss: 1.0250 - acc: 0.5861 - val_loss: 0.9104 - val_acc: 0.6452\n",
      "Epoch 5/50\n",
      "4709/4709 [==============================] - 3s 662us/step - loss: 1.0002 - acc: 0.6099 - val_loss: 0.8723 - val_acc: 0.6545\n",
      "Epoch 6/50\n",
      "4709/4709 [==============================] - 3s 661us/step - loss: 0.9952 - acc: 0.6031 - val_loss: 0.8474 - val_acc: 0.6749\n",
      "Epoch 7/50\n",
      "4709/4709 [==============================] - 3s 661us/step - loss: 0.9668 - acc: 0.6139 - val_loss: 0.8523 - val_acc: 0.6630\n",
      "Epoch 8/50\n",
      "4709/4709 [==============================] - 3s 662us/step - loss: 0.9592 - acc: 0.6233 - val_loss: 0.8660 - val_acc: 0.6613\n",
      "Epoch 9/50\n",
      "4709/4709 [==============================] - 3s 661us/step - loss: 0.9438 - acc: 0.6233 - val_loss: 0.8288 - val_acc: 0.6808\n",
      "Epoch 10/50\n",
      "4709/4709 [==============================] - 3s 661us/step - loss: 0.9328 - acc: 0.6284 - val_loss: 0.8372 - val_acc: 0.6740\n",
      "Epoch 11/50\n",
      "4709/4709 [==============================] - 3s 662us/step - loss: 0.9373 - acc: 0.6296 - val_loss: 0.8357 - val_acc: 0.6715\n",
      "Epoch 12/50\n",
      "4709/4709 [==============================] - 3s 660us/step - loss: 0.9298 - acc: 0.6403 - val_loss: 0.8307 - val_acc: 0.6732\n",
      "Epoch 13/50\n",
      "4709/4709 [==============================] - 3s 660us/step - loss: 0.9262 - acc: 0.6405 - val_loss: 0.8283 - val_acc: 0.6834\n",
      "Epoch 14/50\n",
      "4709/4709 [==============================] - 3s 662us/step - loss: 0.9113 - acc: 0.6441 - val_loss: 0.8115 - val_acc: 0.6876\n",
      "Epoch 15/50\n",
      "4709/4709 [==============================] - 3s 664us/step - loss: 0.9055 - acc: 0.6460 - val_loss: 0.7932 - val_acc: 0.6944\n",
      "Epoch 16/50\n",
      "4709/4709 [==============================] - 3s 663us/step - loss: 0.8899 - acc: 0.6547 - val_loss: 0.7901 - val_acc: 0.7012\n",
      "Epoch 17/50\n",
      "4709/4709 [==============================] - 3s 663us/step - loss: 0.8922 - acc: 0.6509 - val_loss: 0.7775 - val_acc: 0.6986\n",
      "Epoch 18/50\n",
      "4709/4709 [==============================] - 3s 662us/step - loss: 0.8801 - acc: 0.6513 - val_loss: 0.7980 - val_acc: 0.6944\n",
      "Epoch 19/50\n",
      "4709/4709 [==============================] - 3s 663us/step - loss: 0.8899 - acc: 0.6568 - val_loss: 0.7637 - val_acc: 0.7037\n",
      "Epoch 20/50\n",
      "4709/4709 [==============================] - 3s 662us/step - loss: 0.8810 - acc: 0.6526 - val_loss: 0.7945 - val_acc: 0.6919\n",
      "Epoch 21/50\n",
      "4709/4709 [==============================] - 3s 661us/step - loss: 0.8692 - acc: 0.6592 - val_loss: 0.7982 - val_acc: 0.6944\n",
      "Epoch 22/50\n",
      "4709/4709 [==============================] - 3s 660us/step - loss: 0.8685 - acc: 0.6560 - val_loss: 0.7631 - val_acc: 0.7054\n",
      "Epoch 23/50\n",
      "4709/4709 [==============================] - 3s 665us/step - loss: 0.8787 - acc: 0.6585 - val_loss: 0.7984 - val_acc: 0.6995\n",
      "Epoch 24/50\n",
      "4709/4709 [==============================] - 3s 665us/step - loss: 0.8687 - acc: 0.6640 - val_loss: 0.7933 - val_acc: 0.6986\n",
      "Epoch 25/50\n",
      "4709/4709 [==============================] - 3s 665us/step - loss: 0.8643 - acc: 0.6725 - val_loss: 0.7838 - val_acc: 0.6961\n",
      "Epoch 26/50\n",
      "4709/4709 [==============================] - 3s 665us/step - loss: 0.8516 - acc: 0.6677 - val_loss: 0.7700 - val_acc: 0.7088\n",
      "Epoch 27/50\n",
      "4709/4709 [==============================] - 3s 665us/step - loss: 0.8545 - acc: 0.6698 - val_loss: 0.7705 - val_acc: 0.7088\n",
      "Epoch 28/50\n",
      "4709/4709 [==============================] - 3s 665us/step - loss: 0.8538 - acc: 0.6685 - val_loss: 0.7667 - val_acc: 0.7173\n",
      "Epoch 29/50\n",
      "4709/4709 [==============================] - 3s 665us/step - loss: 0.8426 - acc: 0.6776 - val_loss: 0.7536 - val_acc: 0.7139\n",
      "Epoch 30/50\n",
      "4709/4709 [==============================] - 3s 663us/step - loss: 0.8497 - acc: 0.6772 - val_loss: 0.7528 - val_acc: 0.7097\n",
      "Epoch 31/50\n",
      "4709/4709 [==============================] - 3s 664us/step - loss: 0.8435 - acc: 0.6772 - val_loss: 0.7435 - val_acc: 0.7207\n",
      "Epoch 32/50\n",
      "4709/4709 [==============================] - 3s 663us/step - loss: 0.8478 - acc: 0.6691 - val_loss: 0.7587 - val_acc: 0.7088\n",
      "Epoch 33/50\n",
      "4709/4709 [==============================] - 3s 664us/step - loss: 0.8336 - acc: 0.6779 - val_loss: 0.7512 - val_acc: 0.7080\n",
      "Epoch 34/50\n",
      "4709/4709 [==============================] - 3s 664us/step - loss: 0.8221 - acc: 0.6827 - val_loss: 0.7422 - val_acc: 0.7097\n",
      "Epoch 35/50\n",
      "4709/4709 [==============================] - 3s 663us/step - loss: 0.8237 - acc: 0.6808 - val_loss: 0.7487 - val_acc: 0.7088\n",
      "Epoch 36/50\n",
      "4709/4709 [==============================] - 3s 664us/step - loss: 0.8249 - acc: 0.6815 - val_loss: 0.7533 - val_acc: 0.7122\n",
      "Epoch 37/50\n",
      "4709/4709 [==============================] - 3s 668us/step - loss: 0.8261 - acc: 0.6764 - val_loss: 0.7569 - val_acc: 0.7071\n",
      "Epoch 38/50\n",
      "4709/4709 [==============================] - 3s 664us/step - loss: 0.8156 - acc: 0.6853 - val_loss: 0.7665 - val_acc: 0.7054\n",
      "Epoch 39/50\n",
      "4709/4709 [==============================] - 3s 664us/step - loss: 0.8154 - acc: 0.6808 - val_loss: 0.7572 - val_acc: 0.7156\n",
      "Epoch 40/50\n",
      "4709/4709 [==============================] - 3s 664us/step - loss: 0.8197 - acc: 0.6840 - val_loss: 0.7514 - val_acc: 0.7182\n",
      "Epoch 41/50\n",
      "4709/4709 [==============================] - 3s 664us/step - loss: 0.8085 - acc: 0.6957 - val_loss: 0.7331 - val_acc: 0.7301\n",
      "Epoch 42/50\n",
      "4709/4709 [==============================] - 3s 663us/step - loss: 0.8174 - acc: 0.6795 - val_loss: 0.7294 - val_acc: 0.7309\n",
      "Epoch 43/50\n",
      "4709/4709 [==============================] - 3s 663us/step - loss: 0.8087 - acc: 0.6842 - val_loss: 0.7350 - val_acc: 0.7241\n",
      "Epoch 44/50\n",
      "4709/4709 [==============================] - 3s 664us/step - loss: 0.8152 - acc: 0.6836 - val_loss: 0.7345 - val_acc: 0.7207\n",
      "Epoch 45/50\n",
      "4709/4709 [==============================] - 3s 663us/step - loss: 0.8269 - acc: 0.6789 - val_loss: 0.7531 - val_acc: 0.7165\n",
      "Epoch 46/50\n",
      "4709/4709 [==============================] - 3s 664us/step - loss: 0.7974 - acc: 0.6963 - val_loss: 0.7475 - val_acc: 0.7250\n",
      "Epoch 47/50\n",
      "4709/4709 [==============================] - 3s 663us/step - loss: 0.7982 - acc: 0.6948 - val_loss: 0.7389 - val_acc: 0.7258\n",
      "Epoch 48/50\n",
      "4709/4709 [==============================] - 3s 664us/step - loss: 0.7941 - acc: 0.6936 - val_loss: 0.7334 - val_acc: 0.7233\n",
      "Epoch 49/50\n",
      "4709/4709 [==============================] - 3s 663us/step - loss: 0.8175 - acc: 0.6893 - val_loss: 0.7363 - val_acc: 0.7165\n",
      "Epoch 50/50\n",
      "4709/4709 [==============================] - 3s 664us/step - loss: 0.7869 - acc: 0.6959 - val_loss: 0.7385 - val_acc: 0.7267\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=50, verbose=1, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P2uQOL7q6PLI",
    "outputId": "86d0d5c1-570e-444b-f25f-9c04a1f23367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Personal       0.77      0.89      0.82       524\n",
      "Geopolitical       0.57      0.57      0.57       157\n",
      "   Religious       0.57      0.42      0.48       159\n",
      "   Political       0.77      0.69      0.73       360\n",
      "\n",
      "    accuracy                           0.73      1200\n",
      "   macro avg       0.67      0.64      0.65      1200\n",
      "weighted avg       0.72      0.72      0.72      1200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "\n",
    "categories = ['Personal', 'Geopolitical','Religious','Political']\n",
    "print(classification_report(y_test, y_pred, target_names=categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N0K9EI0U6PLI",
    "outputId": "dbc82b1f-c9dd-4906-bb46-c925110d6a6a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    524\n",
       "3    360\n",
       "2    159\n",
       "1    157\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lRTbt2fjnguz"
   },
   "outputs": [],
   "source": [
    "def get_layer_output(layer_name, data):\n",
    "\n",
    "    intermediate_layer_model = keras.Model(inputs=model.input,\n",
    "                                     outputs=model.get_layer(layer_name).output)\n",
    "    return intermediate_layer_model.predict(data)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RkR155Kj6PLI",
    "outputId": "60a742e4-480c-4a23-f1b1-25f98f55effe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding_1/embeddings:0\n",
      "bidirectional_1/forward_lstm_1/kernel:0\n",
      "bidirectional_1/forward_lstm_1/recurrent_kernel:0\n",
      "bidirectional_1/forward_lstm_1/bias:0\n",
      "bidirectional_1/backward_lstm_1/kernel:0\n",
      "bidirectional_1/backward_lstm_1/recurrent_kernel:0\n",
      "bidirectional_1/backward_lstm_1/bias:0\n",
      "bidirectional_2/forward_lstm_2/kernel:0\n",
      "bidirectional_2/forward_lstm_2/recurrent_kernel:0\n",
      "bidirectional_2/forward_lstm_2/bias:0\n",
      "bidirectional_2/backward_lstm_2/kernel:0\n",
      "bidirectional_2/backward_lstm_2/recurrent_kernel:0\n",
      "bidirectional_2/backward_lstm_2/bias:0\n",
      "dense_1/kernel:0\n",
      "dense_1/bias:0\n",
      "dense_2/kernel:0\n",
      "dense_2/bias:0\n"
     ]
    }
   ],
   "source": [
    "names = [weight.name for layer in model.layers for weight in layer.weights]\n",
    "weights = model.get_weights()\n",
    "\n",
    "for name, weight in zip(names, weights):\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4A7VyYpA6PLI"
   },
   "outputs": [],
   "source": [
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "for name, weight in zip(names, weights):\n",
    "    if name == 'bidirectional_1/backward_lstm_1/kernel:0':\n",
    "        kernel_0 = weight\n",
    "    if name == 'bidirectional_1/backward_lstm_1/recurrent_kernel:0':\n",
    "        recurrent_kernel_0 = weight\n",
    "    if name == 'bidirectional_1/backward_lstm_1/bias:0':\n",
    "        bias_0 = weight\n",
    "    elif name == 'dense_1/kernel:0':\n",
    "        output = weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "id": "4LE6JlVDzHF7",
    "outputId": "757b2190-4814-4485-f5ed-d0f69870c328"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel_0 (20, 256)\n",
      "recurrent_kernel_0 (64, 256)\n",
      "bias_0 (256,)\n",
      "output (64, 128)\n"
     ]
    }
   ],
   "source": [
    "print(\"kernel_0\", kernel_0.shape)\n",
    "print(\"recurrent_kernel_0\", recurrent_kernel_0.shape)\n",
    "print(\"bias_0\", bias_0.shape)\n",
    "print(\"output\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "id": "V-NDrc-jo3ur",
    "outputId": "b5bd21bc-e307-4956-c0bb-032f7f203c9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wxh (256, 20)\n",
      "Whh (256, 64)\n",
      "bxh (256,)\n",
      "Why (128, 64)\n"
     ]
    }
   ],
   "source": [
    "Wxh = kernel_0.T  # shape 4d*e\n",
    "Whh = recurrent_kernel_0.T  # shape 4d\n",
    "bxh = bias_0.T  # shape 4d \n",
    "Why = output.T\n",
    "\n",
    "print(\"Wxh\", Wxh.shape)\n",
    "print(\"Whh\", Whh.shape)\n",
    "print(\"bxh\", bxh.shape)\n",
    "print(\"Why\", Why.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mg0xvtSNkrHy"
   },
   "outputs": [],
   "source": [
    "def LRP(target_data, target_class) :\n",
    "\n",
    "    x = get_layer_output('embedding_1', target_data).squeeze(axis=1)\n",
    "    e = x.shape[1]\n",
    "\n",
    "    T = target_data.shape[0]\n",
    "    d = int(256/4)  \n",
    "    C = Why.shape[0]\n",
    "    \n",
    "    idx    = np.hstack((np.arange(0,d), np.arange(2*d,4*d))).astype(int) \n",
    "    idx_i, idx_g, idx_f, idx_o = np.arange(0,d), np.arange(d,2*d), np.arange(2*d,3*d), np.arange(3*d,4*d)\n",
    "\n",
    "    h  = np.zeros((T,d))\n",
    "    c  = np.zeros((T,d))\n",
    "\n",
    "    gates_xh  = np.zeros((T, 4*d))  \n",
    "    gates_hh  = np.zeros((T, 4*d)) \n",
    "    gates_pre = np.zeros((T, 4*d))  \n",
    "    gates     = np.zeros((T, 4*d))  \n",
    "\n",
    "    for t in range(T):\n",
    "        gates_xh[t]     = np.dot(Wxh, x[t])\n",
    "        gates_hh[t]     = np.dot(Whh, h[t-1])\n",
    "        gates_pre[t]    = gates_xh[t] + gates_hh[t] + bxh\n",
    "        gates[t, idx]    = 1.0/(1.0 + np.exp(- gates_pre[t,idx]))\n",
    "        gates[t,idx_g]  = np.tanh(gates_pre[t,idx_g]) \n",
    "        c[t]            = gates[t,idx_f]*c[t-1] + gates[t,idx_i]*gates[t,idx_g]\n",
    "        h[t]            = gates[t,idx_o]*np.tanh(c[t])\n",
    "\n",
    "    s = np.dot(Why, h[t])    \n",
    "\n",
    "    dx     = np.zeros(x.shape)\n",
    "\n",
    "    dh          = np.zeros((T, d))\n",
    "    dc          = np.zeros((T, d))\n",
    "    dgates_pre  = np.zeros((T, 4*d)) \n",
    "    dgates      = np.zeros((T, 4*d))\n",
    "\n",
    "    ds               = np.zeros((C))\n",
    "    ds[target_class] = 1.0\n",
    "    dy               = ds.copy()\n",
    "\n",
    "    dh[T-1]     = np.dot(Why.T, dy)\n",
    "\n",
    "    for t in reversed(range(T)): \n",
    "        dgates[t,idx_o]    = dh[t] * np.tanh(c[t])  \n",
    "        dc[t]             += dh[t] * gates[t,idx_o] * (1.-(np.tanh(c[t]))**2) \n",
    "        dgates[t,idx_f]    = dc[t] * c[t-1]         \n",
    "        dc[t-1]            = dc[t] * gates[t,idx_f] \n",
    "        dgates[t,idx_i]    = dc[t] * gates[t,idx_g]\n",
    "        dgates[t,idx_g]    = dc[t] * gates[t,idx_i]\n",
    "        dgates_pre[t,idx]  = dgates[t,idx] * gates[t,idx] * (1.0 - gates[t,idx]) \n",
    "        dgates_pre[t,idx_g]= dgates[t,idx_g] *  (1.-(gates[t,idx_g])**2) \n",
    "        dh[t-1]            = np.dot(Whh.T, dgates_pre[t])\n",
    "        dx[t]              = np.dot(Wxh.T, dgates_pre[t])\n",
    "\n",
    "    eps=0.001 \n",
    "    bias_factor=0.0\n",
    "    Rx  = np.zeros(x.shape)\n",
    "    Rh  = np.zeros((T+1, d))\n",
    "    Rc  = np.zeros((T+1, d))\n",
    "    Rg  = np.zeros((T,   d))\n",
    "\n",
    "    Rout_mask            = np.zeros((C))\n",
    "    Rout_mask[target_class] = 1.0  \n",
    "\n",
    "    Rh[T-1]  = lrp_linear(h[T-1], Why.T, np.zeros((C)), s, s*Rout_mask, 2*d, eps, bias_factor, debug=False)  \n",
    "\n",
    "    for t in reversed(range(T)):\n",
    "        Rc[t]   += Rh[t]\n",
    "        Rc[t-1]  = lrp_linear(gates[t,idx_f]*c[t-1], np.identity(d), np.zeros((d)), c[t], Rc[t], 2*d, eps, bias_factor, debug=False)\n",
    "        Rg[t]    = lrp_linear(gates[t,idx_i]*gates[t,idx_g], np.identity(d), np.zeros((d)), c[t], Rc[t], 2*d, eps, bias_factor, debug=False)\n",
    "        Rx[t]    = lrp_linear(x[t], Wxh[idx_g].T, bxh[idx_g], gates_pre[t,idx_g], Rg[t], d+e, eps, bias_factor, debug=False)\n",
    "        Rh[t-1]  = lrp_linear(h[t-1], Whh[idx_g].T, bxh[idx_g], gates_pre[t,idx_g], Rg[t], d+e, eps, bias_factor, debug=False)    \n",
    "\n",
    "    return s, dx, Rx, Rh[-1].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OL4JZvkL6PLK"
   },
   "outputs": [],
   "source": [
    "index_word = {v:k for k,v in word_index.items()}\n",
    "\n",
    "def index_to_word(word):\n",
    "    full_sentence = ' '.join(index_word.get(w) for w in word)\n",
    "    return full_sentence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1l5SvryB6PLK"
   },
   "outputs": [],
   "source": [
    "def int_to_str(target_class):\n",
    "    if target_class == 0:\n",
    "        return \"Personal\"\n",
    "    elif target_class == 1:\n",
    "        return \"Political\"\n",
    "    elif target_class == 2:\n",
    "        return \"Religious\"\n",
    "    elif target_class == 3:\n",
    "        return \"Geopolitical\"\n",
    "    else:\n",
    "        return \"Fake\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sSKVxHqZkrAo"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6xaMVDankq9P"
   },
   "outputs": [],
   "source": [
    "per_list = []\n",
    "geo_list = []\n",
    "reli_list = []\n",
    "poli_list = []\n",
    "\n",
    "categories = ['Personal', 'Geopolitical','Religious','Political']\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    if np.argmax(predictions[i]) == 0:\n",
    "        per_list.append(i)\n",
    "    elif np.argmax(predictions[i]) == 1:\n",
    "        geo_list.append(i)\n",
    "    elif np.argmax(predictions[i]) == 2:\n",
    "        reli_list.append(i)\n",
    "    else:\n",
    "        poli_list.append(i)\n",
    "\n",
    "index_word = {v: k for k, v in word_index.items()}\n",
    "seqs = x_test\n",
    "words = []\n",
    "\n",
    "def seqToWords():\n",
    "    for seq in seqs:\n",
    "        if len(seq):\n",
    "            words.append(index_word.get(seq[0]))\n",
    "        else:\n",
    "            words.append(' ')    \n",
    "            \n",
    "    return words\n",
    "\n",
    "\n",
    "words = seqToWords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xsXNyefA6PLL"
   },
   "outputs": [],
   "source": [
    "reverse_word_map = dict(map(reversed, word_index.items()))\n",
    "\n",
    "def sequence_to_text(list_of_indices):\n",
    "    words = [reverse_word_map.get(letter) for letter in list_of_indices]\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lp8fEvobkq7P"
   },
   "outputs": [],
   "source": [
    "def explainer(class_list):   \n",
    "    for index, i in enumerate(class_list):\n",
    "    \n",
    "        target_data = x_test[i]\n",
    "        print(target_data)\n",
    "        print(target_data.shape)\n",
    "        target_class = np.argmax(y_test[i])\n",
    "\n",
    "        scores, Gx, Rx, R_rest = LRP(target_data, target_class)\n",
    "    \n",
    "        R_words          = np.sum(Rx, axis=1)                      \n",
    "        R_words_SA       = (np.linalg.norm(Gx,ord=2, axis=0))**2\n",
    "        R_words_GI       = np.dot(target_data, Gx) \n",
    "    \n",
    "        try:\n",
    "            words = index_to_word(target_data)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        if len(words) > 0 :\n",
    "            print(\"Predicted label:\", int_to_str(np.argmax(predictions[i])), \"Actual label:\", int_to_str(target_class))\n",
    "\n",
    "            print(\"LRP heatmap:\")\n",
    "            display(HTML(html_heatmap(words, R_words)))\n",
    "\n",
    "            print(\"SA heatmap:\")\n",
    "            display(HTML(html_heatmap(words, R_words_SA)))\n",
    "\n",
    "            print(\"GI heatmap:\")\n",
    "            display(HTML(html_heatmap(words, R_words_GI)))\n",
    "\n",
    "            print(\"-----------------------------------------------------------\")\n",
    "\n",
    "        if index == 50: \n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tb593iWL6PLM",
    "outputId": "848755b8-08aa-4d26-ac1e-a8807c6f4aca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The degree to which each word affects the prediction of being personal hates...\n",
      "[ 158 8998    3  158 9065    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "(20,)\n",
      "[ 161   13   22   41 5026  339  139   18 2404   98  189  829    1 1633\n",
      "  105   17 2613 3827   16    0]\n",
      "(20,)\n",
      "[   5  304  894  338  216   86 1841 5266   17 3219 2375  366  351 6035\n",
      " 5879   34  793   76 4356    0]\n",
      "(20,)\n",
      "[  41  418  626 1006 1005    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "(20,)\n",
      "[  20  174   31  174  136   31  177  147    2 1715    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "(20,)\n",
      "[2326 3847 1992 5516  159    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "(20,)\n",
      "[   5  257  111 5066   33  937    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "(20,)\n",
      "[2190  864   16 1819    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "(20,)\n",
      "[  21  476 2186  241  105  259   68    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "(20,)\n",
      "[  20  107  311  109  634   25  679   20 1432  521 2871 7780    0    0\n",
      "    0    0    0    0    0    0]\n",
      "(20,)\n",
      "[ 677  558   84 9951  100 5225  890    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "(20,)\n",
      "[  24 2442 2617 4513  709 1126   35  184  106  914  255 1298   11    0\n",
      "    0    0    0    0    0    0]\n",
      "(20,)\n",
      "[3951 3412  259  139    2 3084    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "(20,)\n",
      "[1623 1077   33   44   11    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "(20,)\n",
      "[ 290  109   50 1853    3  109 2818 1092 5037    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "(20,)\n",
      "[ 572 3588  845  725 1168    2 1911  148    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "(20,)\n",
      "[ 125  182 1739  720 1936 2465    3  125  182  958   60  720  210 2465\n",
      "    0    0    0    0    0    0]\n",
      "(20,)\n",
      "[1197  139  342 1395  575   16    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "(20,)\n",
      "[ 125  744  598   42 4187  102    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "(20,)\n",
      "[807 743 338  88 807 370  30 191 224   8   0   0   0   0   0   0   0   0\n",
      "   0   0]\n",
      "(20,)\n",
      "[3675 1993    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "(20,)\n",
      "[119  79 209  61  43   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0]\n",
      "(20,)\n",
      "[2304 1545   83   83   25   27  111    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "(20,)\n",
      "[1050   15    5  621  120   85    1 1670   75 1216   33   76 5138   61\n",
      "    0    0    0    0    0    0]\n",
      "(20,)\n",
      "[ 266  475  377  181 4668 1219 1170    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "(20,)\n",
      "[ 533   22   17 1977 1092  214 2266    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "(20,)\n",
      "[  12  155  463 1448    2 1421   26  504    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "(20,)\n",
      "[  53  720   75  833  205  118  552   25  300  329 2133   15   52 1129\n",
      " 8705   30 3434  519    0    0]\n",
      "(20,)\n",
      "[4984   85  379 2603  446    4  237    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "(20,)\n",
      "[1667  710 1663 2131 2346   95 1431 1128 2347 1431  706  106    0    0\n",
      "    0    0    0    0    0    0]\n",
      "(20,)\n",
      "[  58 1030 1742  127  505  109   30   27  248  537    4 1444   12  183\n",
      "  248   83 5798    0    0    0]\n",
      "(20,)\n",
      "[ 31   4 249 108   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0]\n",
      "(20,)\n",
      "[ 640 6868 1117 1040    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "(20,)\n",
      "[ 871    4   16  464  913    4 2675 2675  460  871   85   63   32    0\n",
      "    0    0    0    0    0    0]\n",
      "(20,)\n",
      "[  23   17 2019    9   66    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "(20,)\n",
      "[1880  642 4148   34 8873    9    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "(20,)\n",
      "[7924  498  274  484  413   17   14 6317    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "(20,)\n",
      "[ 427 5620 3119   83    2 8421 1272    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "(20,)\n",
      "[  21   48  943 6870  141    4 4190  790  598    2  312    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "(20,)\n",
      "[ 394 1516  394    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "(20,)\n",
      "[1698 1514  322 1447  424  692    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "(20,)\n",
      "[   5  978   63 2238 2238    2  463    6   30 3871  143    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "(20,)\n",
      "[2648    6   30  377   71   11  899 1175   80    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "(20,)\n",
      "[940   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0]\n",
      "(20,)\n",
      "[ 712 1095 1095  174  920 1211 1579    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "(20,)\n",
      "[ 205  733  122  378  186   79 1508  124  375  339 5259  122  392  113\n",
      "    0    0    0    0    0    0]\n",
      "(20,)\n",
      "[ 133  120 1002  418 3505   60   45  132 3846 2217  748    1  161   66\n",
      " 6719    0    0    0    0    0]\n",
      "(20,)\n",
      "[1717  413   92   64  158    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "(20,)\n",
      "[  55   58 3057   93 2008  214  302    3 7705  204  464 3057  290 3710\n",
      "    0    0    0    0    0    0]\n",
      "(20,)\n",
      "[ 168  106   58    5  276 1493 3899   93   91   24  699    1    2 1298\n",
      "  108  231 7892    0    0    0]\n",
      "(20,)\n",
      "[5574  152   56 6932  110  841 2448  270  136    3  136   38 7113    3\n",
      "  362 3062   41   96 3306   96]\n",
      "(20,)\n",
      "Predicted label: Personal Actual label: Personal\n",
      "LRP heatmap:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"background-color:#fefeff\">হচ্ছেন</span> <span style=\"background-color:#fffefe\">মারা</span> <span style=\"background-color:#fffefe\">তাই</span> <span style=\"background-color:#fffefe\">দুজনে</span> <span style=\"background-color:#fefeff\">ঠিক</span> <span style=\"background-color:#fefeff\">করেছেন</span> <span style=\"background-color:#fefeff\">থেকেই</span> <span style=\"background-color:#f6f6ff\">হাত</span> <span style=\"background-color:#fffcfc\">মেরে</span> <span style=\"background-color:#fefeff\">আর</span> <span style=\"background-color:#ffeaea\">মেরে</span> <span style=\"background-color:#ffeeee\">তাদের</span> <span style=\"background-color:#ffdcdc\">জীবনটা</span> <span style=\"background-color:#ffeaea\">আর</span> <span style=\"background-color:#c3c3ff\">করবেন</span> <span style=\"background-color:#ceceff\">তাইতো</span> <span style=\"background-color:#e8e8ff\">এখন</span> <span style=\"background-color:#c0c0ff\">হচ্ছে</span> <span style=\"background-color:#0000ff\">মিথিলা</span> <span style=\"background-color:#ffa6a6\">হচ্ছে</span> \n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SA heatmap:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"background-color:#ffeeee\">হচ্ছেন</span> <span style=\"background-color:#ff7878\">মারা</span> <span style=\"background-color:#ffa2a2\">তাই</span> <span style=\"background-color:#fffafa\">দুজনে</span> <span style=\"background-color:#ff6666\">ঠিক</span> <span style=\"background-color:#ffe6e6\">করেছেন</span> <span style=\"background-color:#ff1c1c\">থেকেই</span> <span style=\"background-color:#ff0000\">হাত</span> <span style=\"background-color:#ffbebe\">মেরে</span> <span style=\"background-color:#ff7676\">আর</span> <span style=\"background-color:#ffb3b3\">মেরে</span> <span style=\"background-color:#ffecec\">তাদের</span> <span style=\"background-color:#fff6f6\">জীবনটা</span> <span style=\"background-color:#ffc3c3\">আর</span> <span style=\"background-color:#ffb0b0\">করবেন</span> <span style=\"background-color:#fff4f4\">তাইতো</span> <span style=\"background-color:#ffbcbc\">এখন</span> <span style=\"background-color:#ff1616\">হচ্ছে</span> <span style=\"background-color:#fffafa\">মিথিলা</span> <span style=\"background-color:#ffe0e0\">হচ্ছে</span> \n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GI heatmap:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"background-color:#ffeeee\">হচ্ছেন</span> <span style=\"background-color:#7c7cff\">মারা</span> <span style=\"background-color:#8383ff\">তাই</span> <span style=\"background-color:#ffd8d8\">দুজনে</span> <span style=\"background-color:#c3c3ff\">ঠিক</span> <span style=\"background-color:#ffbcbc\">করেছেন</span> <span style=\"background-color:#ffe6e6\">থেকেই</span> <span style=\"background-color:#ffcece\">হাত</span> <span style=\"background-color:#acacff\">মেরে</span> <span style=\"background-color:#ff2c2c\">আর</span> <span style=\"background-color:#a6a6ff\">মেরে</span> <span style=\"background-color:#ffe0e0\">তাদের</span> <span style=\"background-color:#c3c3ff\">জীবনটা</span> <span style=\"background-color:#8888ff\">আর</span> <span style=\"background-color:#7979ff\">করবেন</span> <span style=\"background-color:#ffeaea\">তাইতো</span> <span style=\"background-color:#ff8a8a\">এখন</span> <span style=\"background-color:#ff0000\">হচ্ছে</span> <span style=\"background-color:#fff2f2\">মিথিলা</span> <span style=\"background-color:#ff9292\">হচ্ছে</span> \n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"The degree to which each word affects the prediction of being personal hates...\")\n",
    "explainer(per_list)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "raw.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
